{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\co41c\\anaconda3\\envs\\conda1\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# ML models\n",
    "from openxai.LoadModel import LoadModel\n",
    "\n",
    "# Data loaders\n",
    "from openxai.dataloader import return_loaders\n",
    "\n",
    "# Explanation models\n",
    "from openxai.Explainer import Explainer\n",
    "\n",
    "# Evaluation methods\n",
    "from openxai.evaluator import Evaluator\n",
    "\n",
    "# Perturbation methods required for the computation of the relative stability metrics\n",
    "from openxai.explainers.catalog.perturbation_methods import NormalPerturbation\n",
    "from openxai.explainers.catalog.perturbation_methods import NewDiscrete_NormalPerturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model and the data set you wish to generate explanations for\n",
    "data_loader_batch_size = 32\n",
    "data_name = 'compas' # must be one of ['heloc', 'adult', 'german', 'compas']\n",
    "model_name = 'ann'    # must be one of ['lr', 'ann']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Explanation method hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Lime\n",
    "lime_mode = 'tabular'\n",
    "lime_sample_around_instance = True\n",
    "lime_kernel_width = 0.75\n",
    "lime_n_samples = 1000\n",
    "lime_discretize_continuous = False\n",
    "lime_standard_deviation = float(np.sqrt(0.03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loaders\n",
    "loader_train, loader_test = return_loaders(data_name=data_name,\n",
    "                                           download=True,\n",
    "                                           batch_size=data_loader_batch_size)\n",
    "data_iter = iter(loader_test)\n",
    "inputs, labels = data_iter.next()\n",
    "labels = labels.type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get full training data set\n",
    "data_all = torch.FloatTensor(loader_train.dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Load a pretrained ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ml model\n",
    "model = LoadModel(data_name=data_name,\n",
    "                  ml_model=model_name,\n",
    "                  pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Choose an explanation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I: Explanation method with particular hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can supply your own set of hyperparameters like so:\n",
    "param_dict_lime = dict()\n",
    "param_dict_lime['dataset_tensor'] = data_all\n",
    "param_dict_lime['std'] = lime_standard_deviation\n",
    "param_dict_lime['mode'] = lime_mode\n",
    "param_dict_lime['sample_around_instance'] = lime_sample_around_instance\n",
    "param_dict_lime['kernel_width'] = lime_kernel_width\n",
    "param_dict_lime['n_samples'] = lime_n_samples\n",
    "param_dict_lime['discretize_continuous'] = lime_discretize_continuous\n",
    "lime = Explainer(method='lime',\n",
    "                 model=model,\n",
    "                 dataset_tensor=data_all,\n",
    "                 param_dict_lime=param_dict_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 143.56it/s]\n"
     ]
    }
   ],
   "source": [
    "lime_custom = lime.get_explanation(inputs, \n",
    "                                   label=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7237, -0.0166, -0.8009, -1.0835, -0.0397,  0.0293, -0.0266])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_custom[0,:]\n",
    "# print(lime_custom.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II: Explanation method with default hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 113.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6814, -0.0175, -0.6782, -0.8242, -0.0259,  0.0073, -0.0315])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use the default hyperparameters likes so:\n",
    "lime = Explainer(method='lime',\n",
    "                 model=model,\n",
    "                 dataset_tensor=data_all,\n",
    "                 param_dict_lime=None)\n",
    "lime_default_exp = lime.get_explanation(inputs.float(), \n",
    "                                        label=labels)\n",
    "lime_default_exp[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III: Explanation method with default hyperparameters (IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9175, -0.0696, -1.5133, -3.3659, -0.0171,  0.0143, -0.0383],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "# To use a different explanation method change the method name like so\n",
    "ig = Explainer(method='ig',\n",
    "               model=model,\n",
    "               dataset_tensor=data_all,\n",
    "               param_dict_lime=None)\n",
    "ig_default_exp = ig.get_explanation(inputs.float(), \n",
    "                                    label=labels)\n",
    "ig_default_exp[index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV: Explanation method with default hyperparameters (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\co41c\\anaconda3\\envs\\conda1\\lib\\site-packages\\captum\\attr\\_core\\lime.py:1110: UserWarning: You are providing multiple inputs for Lime / Kernel SHAP attributions. This trains a separate interpretable model for each example, which can be time consuming. It is recommended to compute attributions for one example at a time.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3056, -0.1022, -0.2107, -0.0501, -0.0708,  0.0389, -0.0608])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap = Explainer(method='shap',\n",
    "                 model=model,\n",
    "                 dataset_tensor=data_all,\n",
    "                 param_dict_shap=None)\n",
    "shap_default_exp = shap.get_explanation(inputs.float(),\n",
    "                                        label=labels)\n",
    "shap_default_exp[index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Choose an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(explanation, top_k):\n",
    "    mask_indices = torch.topk(explanation, top_k).indices\n",
    "    mask = torch.zeros(explanation.shape) > 10\n",
    "    for i in mask_indices:\n",
    "        mask[i] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation class parameters\n",
    "perturbation_mean = 0.0\n",
    "perturbation_std = 0.10\n",
    "perturbation_flip_percentage = 0.03\n",
    "if data_name == 'compas':\n",
    "    feature_types = ['c', 'd', 'c', 'c', 'd', 'd', 'd']\n",
    "# Adult feature types\n",
    "elif data_name == 'adult':\n",
    "    feature_types = ['c'] * 6 + ['d'] * 7\n",
    "\n",
    "# Gaussian feature types\n",
    "elif data_name == 'synthetic':\n",
    "    feature_types = ['c'] * 20\n",
    "# Heloc feature types\n",
    "elif data_name == 'heloc':\n",
    "    feature_types = ['c'] * 23\n",
    "elif data_name == 'german':\n",
    "    feature_types = pickle.load(open('./data/German_Credit_Data/german-feature-metadata.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation methods\n",
    "if data_name == 'german':\n",
    "    # use special perturbation class\n",
    "    perturbation = NewDiscrete_NormalPerturbation(\"tabular\",\n",
    "                                                  mean=perturbation_mean,\n",
    "                                                  std_dev=perturbation_std,\n",
    "                                                  flip_percentage=perturbation_flip_percentage)\n",
    "\n",
    "else:\n",
    "    perturbation = NormalPerturbation(\"tabular\",\n",
    "                                      mean=perturbation_mean,\n",
    "                                      std_dev=perturbation_std,\n",
    "                                      flip_percentage=perturbation_flip_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = dict()\n",
    "index = index\n",
    "index = 0\n",
    "explainer_name = 'ig'\n",
    "\n",
    "# inputs and models\n",
    "input_dict['x'] = inputs[index].reshape(-1)\n",
    "input_dict['input_data'] = inputs\n",
    "input_dict['explainer'] = ig\n",
    "input_dict['explanation_x'] = ig_default_exp[index,:].flatten()\n",
    "input_dict['model'] = model\n",
    "\n",
    "# perturbation method used for the stability metric\n",
    "input_dict['perturbation'] = perturbation\n",
    "input_dict['perturb_method'] = perturbation\n",
    "input_dict['perturb_max_distance'] = 0.4\n",
    "input_dict['feature_metadata'] = feature_types\n",
    "input_dict['p_norm'] = 2\n",
    "input_dict['eval_metric'] = None\n",
    "\n",
    "# true label, predicted label, and masks\n",
    "input_dict['top_k'] = 3\n",
    "input_dict['y'] = labels[index].detach().item()\n",
    "input_dict['y_pred'] = torch.max(model(inputs[index].unsqueeze(0).float()), 1).indices.detach().item()\n",
    "input_dict['mask'] = generate_mask(input_dict['explanation_x'].reshape(-1), input_dict['top_k'])\n",
    "\n",
    "# required for the representation stability measure\n",
    "input_dict['L_map'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(input_dict,\n",
    "                      inputs=inputs,\n",
    "                      labels=labels, \n",
    "                      model=model, \n",
    "                      explainer=ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model, 'return_ground_truth_importance'):\n",
    "    rc = evaluator.evaluate(metric='RC')\n",
    "    fa = evaluator.evaluate(metric='FA')\n",
    "    ra = evaluator.evaluate(metric='RA')\n",
    "    sa = evaluator.evaluate(metric='SA')\n",
    "    sra = evaluator.evaluate(metric='SRA')\n",
    "    # evaluate rank correlation\n",
    "    print('RC:', rc)\n",
    "\n",
    "    # evaluate feature agreement\n",
    "    print('FA:', fa)\n",
    "\n",
    "    # evaluate rank agreement\n",
    "    print('RA:', ra)\n",
    "\n",
    "    # evaluate sign agreement\n",
    "    print('SA:', sa)\n",
    "\n",
    "    # evaluate signed rankcorrelation\n",
    "    print('SRA:', sra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 132.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 120.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 122.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 108.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 110.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 123.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 153.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 109.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 153.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 110.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 110.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 104.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 132.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 104.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 104.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 140.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 104.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 120.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 116.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 141.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 132.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 132.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 140.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 132.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 99.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 120.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 132.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 153.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 129.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 132.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 86.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 95.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 94.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 76.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 95.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 94.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 99.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 108.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 132.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 77.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGU: 0.40282378\n",
      "PGI: 0.49907628\n",
      "RIS: 172.68180765794202\n",
      "ROS: 10295.997900091292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate prediction gap on umportant features\n",
    "pgu = evaluator.evaluate(metric='PGU')\n",
    "pgi = evaluator.evaluate(metric='PGI')\n",
    "ris = evaluator.evaluate(metric='RIS')\n",
    "ros = evaluator.evaluate(metric='ROS')\n",
    "print('PGU:', pgu)\n",
    "\n",
    "# evaluate prediction gap on important features\n",
    "print('PGI:', pgi)\n",
    "\n",
    "# evaluate relative input stability\n",
    "print('RIS:', ris)\n",
    "\n",
    "# evaluate relative output stability\n",
    "print('ROS:', ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_results = {}\n",
    "field_names = ['dataset', 'model', 'explainer', 'RC', 'FA', 'RA', 'SA', 'SRA', 'PGU', 'PGI', 'RIS', 'ROS']\n",
    "csv_results['dataset'] = data_name\n",
    "csv_results['model'] = model_name\n",
    "csv_results['explainer'] = explainer_name\n",
    "if hasattr(model, 'return_ground_truth_importance'):\n",
    "    csv_results['RC'] = rc\n",
    "    csv_results['FA'] = fa\n",
    "    csv_results['RA'] = ra\n",
    "    csv_results['SA'] = sa\n",
    "    csv_results['SRA'] = sra\n",
    "else:\n",
    "    csv_results['RC'] = ''\n",
    "    csv_results['FA'] = ''\n",
    "    csv_results['RA'] = ''\n",
    "    csv_results['SA'] = ''\n",
    "    csv_results['SRA'] = ''\n",
    "csv_results['PGU'] = pgu\n",
    "csv_results['PGI'] = pgi\n",
    "csv_results['RIS'] = ris\n",
    "csv_results['ROS'] = ros\n",
    "\n",
    "with open('results.csv', 'a', newline='') as csv_file:\n",
    "    dict_obj = csv.DictWriter(csv_file, fieldnames=field_names)\n",
    "    dict_obj.writerow(csv_results)\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('conda1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac86d61d6cbb0c2c1a60a10445e5164ec809b444a240941ceee690f0d57a9742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
